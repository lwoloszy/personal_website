Title: Bias-variance tradeoff visualized
Date: 2016-6-28
Tags:  bias-variance tradeoff, frequentist statistics
Category: Machine learning
Slug: bias-variance-tradeoff-visualized
Summary: Graphical representation of how regularization affects bias-variance tradeoff
Status: 

Overview
--------
In supervised machine learning, a common issue in model selection is the bias-variance tradeoff. The bias-variance tradeoff refers to the decomposition of the squared prediction error into two components: bias and variance. Here I want to provide a visually compelling intuition for this tradeoff. Before I start, I'd like to acknowledge that much of this post was heavily inspired by portions of Chapter 3 of Chris Bishop's excellent book _Pattern Recognition and Machine Learning_. In fact, I tried to nearly replicate some figures from that chapter in order to convince myself that the simulations I performed were correct (I'm a stickler for getting that 5th digit after the decimal on the money). But if you'd like to play around with other functions, take a look at the code I wrote [here](https://github.com/lwoloszy/demos/blob/master/bias_variance.py). It should be straightforward to generate novel graphs.

Background
----------
Before I get to the graphical stuff, I want to quickly go over some theory. At their core, problems in supervised machine learning are problems of function approximation. Given a set of training inputs and their outputs, we would like to learn a function that generalizes from the data in such a way that we can predict outputs for inputs that we haven't seen before. Let's assume that the true underlying relationship between inputs and outputs is: $$y = f(x) + \epsilon$$ where $\epsilon$ is some amount of irreducible noise in the mapping from $x$ to $y$. The goal of a learning algorithm then is to learn a function $\hat{f}(x; \mathcal{D})$ that as closely as possible approximates the true function $f(x)$. Notice that I've made explicit, via the use of $\mathcal{D}$, the dependence of $\hat{f}$ on the particular dataset used to train the algorithm. We'll come back to this in just a moment. 

A sensible and convenient metric to assess the goodness of fit of a model is to compute the squared distance between the predicted output $\hat{f}(x; \mathcal{D})$ and the actual output $y$ over all possible values of $x$: $$\int \{\hat{f}(x; \mathcal{D}) - y\}^2\ p(x)\ dx$$ The quality of a model is often judged by how small it can make this quantity: the smaller the squared error, the better the model. But another question worth asking is how certain we are of the model we learned. Because $\hat{f}$ is a function of the particular training dataset used, it is a random variable. If we were to draw a different set of training data, $\hat{f}$ would be different. So here's a simple thought experiment: let's imagine that we can in fact generate a new dataset and that we then use this new dataset to learn a new model. Let's take this one step further and imagine that we can somehow get our hands on all possible training datasets. We can then use each training dataset to retrain our learning algorithm and arrive at a new $\hat{f}$. In fact, we now have all the possible functions that our learning algorithm could have ever learnt. With all these $\hat{f}$ in hand, let's see how accurate the model's predictions are _on average_. We can denote the average prediction error over the ensemble of functions as: $$\mathbb{E}_\mathcal{D}\big[\big\{\hat{f}(x; \mathcal{D}) - y\big\}^2\big]$$ It should be emphasized that this expectation is really just a theoretical construct - we never get to observe anything close to it. In fact, for the most part, we only ever get to see one training dataset. But let's for the moment carry on with the thought experiment and see whether we can make sense of this quantity.

This expectation can be broken down, with some algebraic manipulation, into three components:

1. $\displaystyle\int \mathbb{E}_\mathcal{D}\big[\big\{\hat{f}(x; \mathcal{D}) - \mathbb{E}_\mathcal{D}[\hat{f}(x; \mathcal{D})]\big\}^2\big]\ p(x)\ dx$ - variance

2. $\displaystyle\int \big\{\mathbb{E}_\mathcal{D}[\hat{f}(x; \mathcal{D})] - f(x)\big\}^2\ p(x)\ dx$ - bias squared

3. $\displaystyle\int \big\{y - f(x)\big\}^2\ p(x, y)\ dy\ dx$ - irreducible noise

The first term represents the variance of the estimator: how far, on average, is any single learned model from the average model. Essentially, the variance term measures how much the model fluctuates with different draws of training data. The second term represents the squared bias: how far is the average model from the true function. If the bias is non-zero, then the model we are using to make predictions will never be able to capture the entire truth (although this does not mean it won't make better predictions than a model with zero bias). And the third term represents the irreducible noise that no estimator will be able to get rid of (it does not depend on the estimator).

What is the bias-variance tradeoff? Well, as the name implies, it turns out that when you increase the variance of the learning algorithm, you decrease its bias, and vice versa. Depending on your use case, one form of error may be more tolerable than another, and the bias-variance formulation grants you access to a knob that you can turn in the direction you prefer. For instance, if you're trying to infer the underlying cause of crime in Chicago, then you probably want your model to have zero bias, even though the variance of the estimator might be really high. On the other hand, if all you care about is prediction, like whether a credit card transaction was fraudulent or not, you can usually sacrifice zero bias in favor of lower variance, which will often lead to better predictive accuracy. In general, people more interested in performing causal inference will favor lower bias, whereas people more interested in predictive accuracy will favor lower variance.

Example with visualization
-----------------------------------
To illustrate the tradeoff between bias and variance, let's work with a concrete function that we will try to approximate. I've chosen to use the same function as the _Pattern Recogntion and Machine Learning_ book so that I could verify that the results I got were correct: $$y = sin(2\pi x) + \mathcal{N}(0, .09)$$ where you'll notice we have assumed that the irreducible noise is Gaussian with $\mu = 0$ and $\sigma^2 = .09$. In the left panel of the figure below, I have plotted $sin(2\pi x)$ as a solid line and random samples from the generating distribution as circles, where values over $x$ are uniformly distributed over the interval $0$ to $1$. In the right panel of the same figure, I have drawn the full probability density function (pdf) of the distribution, with darker shades indicating a higher value of the pdf. You can see from this figure that $y$ values close to the mean are more likely than values farther away.

<center> *Figure 1: Plot of the sine function we will try to approximate (left) and the full pdf (right)* </center>

![Sine function]({filename}/images/1-function.png)

Ok, so with this known underlying function in place, we would like to simulate an experiment that will give us insight into the bias-variance tradeoff.  Let's try to estimate this function with a linear model. In particular we assume that the $y$ can be modeled as a linear combination of radial basis functions: $$y = \sum_{i=1}^{m} \beta_i\phi_i(x),$$
where $m$ is the number of basis function in which $x$ is expanded and $\beta_i$ are the coefficients that we will try to learn. Radial basis functions take the form $\phi_i = e^{\frac{(-x - u_i)^{2}}{2s^{2}}}$, where the $u_i$ determine the location and the $s$ determine the spatial scale. I have chosen to expand each $x$ with a set of 24 RBFs whose location parameters are evenly spaced between $0$ and $1$ with $s$ equal to $.075$. In the figure below, I have drawn each of the RBFs as well as as the values that $x=0.25$ would take in this 24-dimensional space when expanded.

<center> *Figure 2: A set of 24 radial basis functions we use to represent each x and one example expansion (x=.25)* </center>

![RBFs]({filename}/images/1-rbfs.png)

In order to fit a set of training data to the above model, we need a cost function to minimize. We will minimize the sum of squares:$$Cost = \frac{1}{2}\sum_{n=1}^{N}{\{y_n - \boldsymbol{w}^{T}\boldsymbol{\phi}(x_n)}\}^{2}$$ where you'll notice I have put all the $\beta_i$ coefficients into the vector $\boldsymbol{w}$ and the expanded $x$ into the vector $\boldsymbol{\phi}.$ (I've also included an intercept term, so even though we have 24 RBFs, the dimensionality of the fitted vector is 25). In order to prevent overfitting, it is common to combine this cost function with a penalty term that prevents the learned coefficients from growing too large. The total cost function then becomes: $$Cost = \frac{1}{2}\sum_{n=1}^{N}{\{y_n - \boldsymbol{w}^{T}\boldsymbol{\phi}(x_n)}\}^{2} + \frac{\lambda}{2}\boldsymbol{w}^T\boldsymbol{w}$$ with $\lambda$ governing how much to penalize large coefficients. It is this $\lambda$ that serves as the knob that we can turn in order to trade off bias for variance. It turns out that lower values of $\lambda$ reduce bias but increase variance, whereas higher values of $\lambda$ increase bias but reduce variance. There's usually a sweet spot where an optimal magnitude of $\lambda$ minimizes the prediction error.

With that out of the way, let's learn and plot some functions! In order to highlight how the $\lambda$ controls the bias-variance tradeoff, I took a random set of 25 points from the function mentioned above and then fitted 3 separate linear models, each with a different value of $\lambda$. I then repeated this a 100 times to see how the fits vary as a function of the particular training dataset and also to illustrate how the average learned function begins to approach the true function. Let's examine in detail the figure shown below.

*Figure 3: Fits of RBF linear model with 3 different $\lambda$. First column shows 20 random fits, second column shows the standard deviation across the predicted functions, and the third column shows the average prediction function and the true function*

![Ind fits]({filename}/images/1-ind_fits.png)

In the first column, I have plotted for each of the 3 $\lambda$ values 20 separate fits of the RBF linear model (I did not plot all 100 fits so that we can see what's going on). Remember that each fit was done on a different set of training data. We can see that as $\lambda$ decreases, the size of the fluctuations in the fitted functions increases. Basically, the functions adapt really well to the particular training dataset that they saw, but at the cost of a lot of variability from one fit to the next. This variance is plotted directly in the middle column, where the width of the ribbon represents the standard deviation across the 100 fits. Now, as I've mentioned, models with a lot of variance typically have low bias, which leads them, on average, to approach the true functions. This is shown in the third column. In the red color we have the average prediction function learned over the 100 fits and in blue we have the true function. We can see that when $\lambda$ is low, the bias is low. The two functions are essentially on top of one another. Conversely, when $\lambda$ is high, the variance across fits is kept at bay but the average learned function will never approximate the true function.

Recall the integral formulas for the variance and bias squared. We can approximate them for this example by computing finite sums. I'll omit the details, but in the final figure below I have plotted the variance, bias squared, and their sum as a function of $\lambda$. I hope you'll appreciate how there seems to be a sweet spot where the prediction error is minimized: stray too far to the left (low $\lambda$) and the fitted model is influenced too much by the given training dataset and exhibits a lot of variance; stray too far to the right (high $\lambda$) and the model is not complex enough (its parameters are clamped at too low a value) for it to be able to explain the underlying function.

<center> *Figure 4: Bias squared, variance and bias squared + variance as a function of $\lambda$* </center>

![Bias-variance decomposition]({filename}/images/1-bv.png)

And that's the bias-variance tradeoff in a nutshell. I hope this was helpful. The source code for the simulations can be found [here](https://github.com/lwoloszy/demos/blob/master/bias_variance.py).
